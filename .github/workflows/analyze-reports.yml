name: Analyze Google Ads Reports per Account (AI)

on:
  workflow_dispatch:      # podes executar manualmente
  schedule:
    - cron: "0 8 * * 1"   # segundas √†s 08:00 UTC (depois do update-report)

jobs:
  analyze-reports:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install openai pandas numpy

      - name: Generate per-account AI insights
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python - << 'EOF'
          import os, json, math
          from pathlib import Path
          import pandas as pd
          import numpy as np
          from openai import OpenAI

          ROOT = Path(".")
          REPORTS_DIR = ROOT / "ads_reports"
          OUT_DIR = ROOT / "ads_ai_insights"
          OUT_DIR.mkdir(parents=True, exist_ok=True)

          # 1) Ler accounts.json
          accounts_path = ROOT / "accounts.json"
          if not accounts_path.exists():
            raise FileNotFoundError("accounts.json n√£o encontrado na raiz do reposit√≥rio.")
          accounts = json.loads(accounts_path.read_text(encoding="utf-8"))

          client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
          if not os.environ.get("OPENAI_API_KEY"):
            raise RuntimeError("OPENAI_API_KEY n√£o definido nos Secrets.")

          def safe_div(a, b):
              return (a / b) if (b and b != 0) else 0.0

          def load_account_df(acc):
              # tenta por nome (pluralcare.json) sen√£o por id (6019645403.json)
              fname_by_name = (REPORTS_DIR / f"{acc['name'].lower().replace(' ','_')}.json")
              fname_by_id   = (REPORTS_DIR / f"{acc['id']}.json")
              if fname_by_name.exists():
                  raw = json.loads(fname_by_name.read_text(encoding="utf-8"))
                  source = fname_by_name.name
              elif fname_by_id.exists():
                  raw = json.loads(fname_by_id.read_text(encoding="utf-8"))
                  source = fname_by_id.name
              else:
                  raise FileNotFoundError(f"Nenhum relat√≥rio JSON encontrado para {acc['name']} ({acc['id']}). Esperado {fname_by_name.name} ou {fname_by_id.name}")

              df = pd.DataFrame(raw or [])
              if df.empty:
                  # garantir colunas
                  df = pd.DataFrame(columns=[
                      "campaign_id","campaign_name","adgroup_id","adgroup_name",
                      "keyword","clicks","impressions","ctr","avg_cpc","cost","date"
                  ])
              # tipos num√©ricos seguros
              for c in ["clicks","impressions","ctr","avg_cpc","cost"]:
                  if c in df.columns:
                      df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0)
              return df, source

          def summarize_account(df):
              # agrega√ß√µes √∫teis
              camp = df.groupby(["campaign_id","campaign_name"], dropna=False).agg(
                  clicks=("clicks","sum"),
                  impressions=("impressions","sum"),
                  cost=("cost","sum")
              ).reset_index()
              camp["ctr"] = camp.apply(lambda r: safe_div(r["clicks"], r["impressions"])*100, axis=1)
              camp["cpc"] = camp.apply(lambda r: safe_div(r["cost"], r["clicks"]), axis=1)
              camp["cpm"] = camp.apply(lambda r: safe_div(r["cost"], r["impressions"])*1000, axis=1)

              kw = df.groupby(["keyword","campaign_name","adgroup_name"], dropna=False).agg(
                  clicks=("clicks","sum"),
                  impressions=("impressions","sum"),
                  cost=("cost","sum")
              ).reset_index().sort_values("cost", ascending=False)
              kw["ctr"] = kw.apply(lambda r: safe_div(r["clicks"], r["impressions"])*100, axis=1)
              kw["cpc"] = kw.apply(lambda r: safe_div(r["cost"], r["clicks"]), axis=1)

              totals = {
                  "clicks": int(df["clicks"].sum()),
                  "impressions": int(df["impressions"].sum()),
                  "cost": float(df["cost"].sum()),
                  "ctr": safe_div(df["clicks"].sum(), df["impressions"].sum())*100 if df["impressions"].sum() else 0.0,
                  "cpc": safe_div(df["cost"].sum(), df["clicks"].sum()) if df["clicks"].sum() else 0.0,
                  "cpm": safe_div(df["cost"].sum(), df["impressions"].sum())*1000 if df["impressions"].sum() else 0.0,
              }

              # amostras para o prompt (limitar tamanho)
              top_campaigns = camp.sort_values("cost", ascending=False).head(20)
              worst_ctr_kw = kw[(kw["impressions"]>=200) & (kw["clicks"]==0)].sort_values("impressions", ascending=False).head(30)
              top_cost_kw  = kw.head(40)

              # serializar para texto compacto
              def df2list(d):
                  # reduzir precis√£o
                  out = d.copy()
                  for col in ["ctr","cpc","cpm","cost"]:
                      if col in out.columns:
                          out[col] = out[col].astype(float).round(4)
                  # converter para listas de dicts
                  return out.to_dict(orient="records")

              payload = {
                  "totals": totals,
                  "top_campaigns": df2list(top_campaigns),
                  "top_cost_keywords": df2list(top_cost_kw),
                  "zero_click_high_impr_keywords": df2list(worst_ctr_kw),
              }
              return payload

          def render_prompt(acc, source_file, payload):
              # instru√ß√µes super detalhadas para extrair valor m√°ximo
              return f"""
Tu √©s um consultor s√©nior de Google Ads. Recebeste dados da conta **{acc['name']}** (ID {acc['id']}), √∫ltimos 30 dias, extra√≠dos do ficheiro {source_file}.
N√ÉO inventes m√©tricas que n√£o existem no dataset (p.ex. convers√µes). Baseia-te apenas em cliques, impress√µes, CTR, CPC, CPM, custo, campanhas, grupos de an√∫ncios e keywords.

### Dados consolidados
{json.dumps(payload['totals'], ensure_ascii=False, indent=2)}

### Top campanhas por custo (m√°x 20)
{json.dumps(payload['top_campaigns'], ensure_ascii=False, indent=2)}

### Palavras-chave com maior custo (m√°x 40)
{json.dumps(payload['top_cost_keywords'], ensure_ascii=False, indent=2)}

### Palavras-chave com MUITAS impress√µes e 0 cliques (candidatas a pausar/negativar)
{json.dumps(payload['zero_click_high_impr_keywords'], ensure_ascii=False, indent=2)}

### Tarefas
1) **Diagn√≥stico profundo** (bullet points) sobre desperd√≠cio, oportunidades e gargalos: por campanhas e por keywords.
2) **Plano de a√ß√£o priorizado (P1, P2, P3)** com tarefas concretas e crit√©rios de decis√£o (p.ex., pausar keywords com >300 impress√µes e CTR <0,3%, reduzir lance em X% onde CPC > m√©dia+20%, realocar or√ßamento de campanhas com CPC alto para as com CPC baixo e CTR acima da mediana, etc.).
3) **Lista de keywords a pausar/ajustar** (com justifica√ß√£o). Se faltarem dados, indica como os recolher.
4) **Sugest√µes de negativas** (parciais/exatas) a partir das keywords com altas impress√µes e 0 cliques (n√£o inventes termos fora do que tens).
5) **Ajustes de lances** (guidelines): por CPC/CTR, com thresholds objetivos (valores).
6) **Or√ßamenta√ß√£o**: onde cortar e onde investir (quantifica percentagens).
7) **Copy de an√∫ncios**: 3 varia√ß√µes de headlines e descri√ß√µes inspiradas nas melhores keywords (usa linguagem persuasiva e PT-PT).
8) **Scorecard** final com 5 KPIs a seguir na pr√≥xima semana (e metas num√©ricas).

Formata em **Markdown**, com sec√ß√µes e tabelas. S√™ espec√≠fico, cr√≠tico e acion√°vel. Portugu√™s de Portugal.
"""

          generated = []

          for acc in accounts:
              try:
                  df, src = load_account_df(acc)
                  payload = summarize_account(df)
                  prompt = render_prompt(acc, src, payload)

                  # Chamada ao modelo
                  resp = client.responses.create(
                      model="gpt-4.1-mini",
                      input=prompt,
                  )
                  text = resp.output_text

                  out_path = OUT_DIR / f"{acc['name'].lower().replace(' ','_')}_insights.md"
                  out_path.write_text(text, encoding="utf-8")
                  print(f"‚úÖ Gerado: {out_path}")
                  generated.append(out_path.name)

              except Exception as e:
                  print(f"‚ö†Ô∏è Erro na conta {acc.get('name')} ({acc.get('id')}): {e}")

          # √çndice agregador
          if generated:
              index_path = OUT_DIR / "README.md"
              lines = ["# Relat√≥rios de Insights por Conta\n"]
              for g in generated:
                  lines.append(f"- [{g}]({g})")
              index_path.write_text("\n".join(lines), encoding="utf-8")
              print(f"üìö √çndice criado: {index_path}")

          print("üèÅ Conclu√≠do.")
          EOF

      - name: Commit and push AI insights
        run: |
          git config user.name "github-actions"
          git config user.email "actions@github.com"
          git add ads_ai_insights/*.md || echo "Sem altera√ß√µes para adicionar"
          git commit -m "Add per-account AI insights" || echo "Sem altera√ß√µes para commit"
          git push
